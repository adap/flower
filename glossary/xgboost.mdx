---
title: "XGBoost"
description: "XGBoost is an open-source library for training gradient-boosted decision trees (GBDTs), widely used for tabular machine learning."
date: "2024-09-10"
author:
  name: "Chong Shen Ng"
  position: "Research Engineer @ Flower Labs"
  website: "https://discuss.flower.ai/u/chongshenng"
  github: "github.com/chongshenng"
related:
  - text: "Federated Gradient Boosting"
    link: "/glossary/federated-gradient-boosting"
  - text: "Cross-silo Federated Learning"
    link: "/glossary/cross-silo-federated-learning"
  - text: "Strategy"
    link: "/glossary/strategy"
  - text: "Quickstart Federated Learning with XGBoost and Flower"
    link: "/docs/framework/tutorial-quickstart-xgboost.html"
  - text: "Flower Example using XGBoost (Comprehensive)"
    link: "/docs/examples/xgboost-comprehensive.html"
---

XGBoost is an open-source library for training gradient-boosted decision tree models (GBDTs). It is widely used on structured/tabular data because it is efficient, often strong out of the box, and provides interpretable tree ensembles.

## Why it matters
- GBDTs are a strong baseline for many tabular problems, common in cross-silo settings (healthcare, finance, industry).
- Training and inference are often practical on modest compute, making them attractive for production workflows.
- Tree ensembles can provide useful interpretability compared to some deep learning approaches.

## In federated settings
Federated training for tree ensembles typically does not look like parameter averaging used for neural networks. Depending on the method, participants may share trees, split decisions, or aggregated statistics (for example histograms) needed to construct trees without revealing raw records. See [Federated Gradient Boosting](/glossary/federated-gradient-boosting) for the concept-level view.

Because tabular features and preprocessing must match across participants, federated gradient boosting is often used in [cross-silo federated learning](/glossary/cross-silo-federated-learning) where parties can coordinate schema and governance more explicitly.

## Common pitfalls
- Assuming you can “just FedAvg” a tree model; the aggregation protocol is algorithm-specific.
- Mismatched feature engineering or categorical encoding across participants that breaks training or evaluation.
- Overlooking leakage risk: shared statistics can still be sensitive without explicit privacy design.
- Reporting a single average metric and missing cohort or site-level performance issues.

## In Flower
In Flower, XGBoost-based federated training is implemented as a [Strategy](/glossary/strategy) inside a [ServerApp](/glossary/serverapp) and can be exercised in the [simulation runtime](/glossary/simulation-runtime) or the [deployment runtime](/glossary/deployment-runtime), alongside the general pattern described in [Federated Gradient Boosting](/glossary/federated-gradient-boosting).
