---
title: "Heterogeneity in Federated Learning"
description: "Heterogeneity describes how clients differ in data, resources, and behavior—one of the core challenges in federated systems."
date: "2024-05-24"
author:
  name: "Adam Narożniak"
  position: "ML Engineer at Flower Labs"
  website: "https://discuss.flower.ai/u/adam.narozniak/summary"
related:
  - text: "Non-IID Data"
    link: "/glossary/non-iid-data"
  - text: "Cross-device Federated Learning"
    link: "/glossary/cross-device-federated-learning"
  - text: "Cross-silo Federated Learning"
    link: "/glossary/cross-silo-federated-learning"
  - text: "FedProx"
    link: "/glossary/fedprox"
  - text: "SCAFFOLD"
    link: "/glossary/scaffold"
  - text: "Personalized Federated Learning (pFL)"
    link: "/glossary/personalized-federated-learning"
  - text: "Flower Datasets"
    link: "/glossary/flower-datasets"
---

Heterogeneity describes the many ways clients in a federation can differ—from their data distributions to their hardware and availability. It is one of the defining challenges of federated learning because these differences can destabilize optimization and hide uneven performance across clients.

## Why it matters
Federated systems rarely have “identical” clients. Heterogeneity can slow convergence, increase variance between rounds, and lead to global models that perform well on average but poorly for specific cohorts.

## In federated settings
It is useful to distinguish three broad sources of heterogeneity:

- **Statistical heterogeneity (non-IID data)**: client datasets differ. Common patterns include label skew, feature/covariate shift, quantity skew (different dataset sizes), and distribution shift over time.
- **System heterogeneity**: clients differ in compute, memory, network bandwidth, and availability. This can create stragglers, dropouts, and uneven training progress.
- **Non-stationarity**: client data-generating processes change over time (for example concept drift), which can invalidate assumptions made during training or evaluation.

Real deployments often exhibit combinations of all three.

## Common pitfalls
- Treating heterogeneity as purely a data problem. System and availability differences can dominate real-world behavior.
- Using only aggregate metrics; heterogeneity often shows up as tail failures (worst-client or cohort regressions).
- Applying “robust” aggregation without considering that legitimate non-IID updates may look like outliers.

## In Flower
In Flower, heterogeneity can be studied in both the [simulation runtime](/glossary/simulation-runtime) and the [deployment runtime](/glossary/deployment-runtime), with statistical heterogeneity often modeled via [Flower Datasets](/glossary/flower-datasets). Server-side mitigations are typically encoded in the [Strategy](/glossary/strategy).
