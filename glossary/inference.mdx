---
title: "Inference"
description: "Inference is the phase in which a trained machine learning model applies its learned patterns to new, unseen data to make predictions or decisions."
date: "2024-07-12"
author:
  name: "Yan Gao"
  position: "Research Scientist"
  website: "https://discuss.flower.ai/u/yan-gao/"
  github: "github.com/yan-gao-GY"
related:
  - text: "Local Model"
    link: "/glossary/local-model"
  - text: "Personalized Federated Learning (pFL)"
    link: "/glossary/personalized-federated-learning"
  - text: "Federated Learning"
    link: "/glossary/federated-learning"
---

Inference is the stage where a trained model is used to make predictions or decisions on new inputs. It typically involves loading the model, applying the same preprocessing as in training, and running a forward pass to produce outputs.

## Why it matters
Inference is where model value is realized in production. Latency, reliability, and resource usage at inference time often matter as much as training accuracy.

## In federated settings
Federated learning affects how models are trained, but inference can happen centrally, on-device, or in a hybrid way. A common pattern is to deploy a **global model** (or a client-specific **local model**) to client environments for on-device inference, reducing data movement and potentially improving latency.

## Common pitfalls
- **Training/inference mismatch**: inconsistent preprocessing or feature availability between training and inference leads to degraded performance.
- **Distribution shift**: client environments can change over time; monitoring and re-training strategies matter.
- Assuming on-device inference implies privacy guarantees; it depends on the full system design and what data leaves the device.

## In Flower
In Flower, inference typically lives in application code (often alongside a [ClientApp](/glossary/clientapp)) using model artifacts produced by a federated run; Flower primarily orchestrates training, evaluation, and analytics.
