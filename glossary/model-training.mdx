---
title: "Model Training"
description: "Model training fits model parameters to data; in federated settings it is executed locally on clients and coordinated across rounds."
date: "2024-07-12"
author:
  name: "Yan Gao"
  position: "Research Scientist"
  website: "https://discuss.flower.ai/u/yan-gao/"
  github: "github.com/yan-gao-GY"
related:
  - text: "Federated Learning"
    link: "/glossary/federated-learning"
  - text: "Round"
    link: "/glossary/round"
  - text: "Federated Averaging (FedAvg)"
    link: "/glossary/fedavg"
  - text: "FedProx"
    link: "/glossary/fedprox"
  - text: "SCAFFOLD"
    link: "/glossary/scaffold"
  - text: "Non-IID Data"
    link: "/glossary/non-iid-data"
  - text: "Heterogeneity in Federated Learning"
    link: "/glossary/heterogeneity-in-federated-learning"
---

Model training is the process of fitting a model’s parameters to data by optimizing an objective (loss). In deep learning, this typically means iteratively updating parameters using gradient-based optimization.

## Why it matters
Training choices (data, model architecture, optimizer, hyperparameters) determine whether a model is accurate, robust, and suitable for production constraints.

## In federated settings
In federated learning, training is executed **locally on clients** and coordinated across **rounds**. A typical round: clients receive the current global model, train locally for a bounded amount of work (for example a few epochs), and return a **model update** to the server for aggregation.

Compared to centralized training, federated training must handle:
- **non-IID data** across clients
- system heterogeneity (different compute/network/availability)
- partial participation and client dropouts

These factors can cause “client drift” and instability; methods like **FedProx** or **SCAFFOLD** are examples of approaches that aim to mitigate such issues.

## Common pitfalls
- Using too many local epochs can reduce communication but worsen convergence under strong non-IID data.
- Reporting only an average metric can hide poor performance for specific clients/cohorts.
- Assuming privacy is guaranteed because data stays local; privacy requires explicit mechanisms and a clear threat model.

## In Flower
In Flower, model training is implemented in [ClientApp](/glossary/clientapp) code and orchestrated by a server-side [Strategy](/glossary/strategy) within a [ServerApp](/glossary/serverapp), in both the [simulation runtime](/glossary/simulation-runtime) and the [deployment runtime](/glossary/deployment-runtime).
