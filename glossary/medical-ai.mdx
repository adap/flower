---
title: "Medical AI"
description: "Medical AI applies machine learning and AI methods to clinical and healthcare tasks such as diagnosis support, imaging, and patient monitoring."
date: "2024-07-12"
author:
  name: "Yan Gao"
  position: "Research Scientist"
  website: "https://discuss.flower.ai/u/yan-gao/"
  github: "github.com/yan-gao-GY"
related:
  - text: "Federated Learning"
    link: "/glossary/federated-learning"
  - text: "Cross-silo Federated Learning"
    link: "/glossary/cross-silo-federated-learning"
  - text: "Federated Evaluation"
    link: "/glossary/federated-evaluation"
  - text: "Secure Aggregation"
    link: "/glossary/secure-aggregation"
  - text: "Differential Privacy (DP)"
    link: "/glossary/differential-privacy"
  - text: "Threat Model"
    link: "/glossary/threat-model"
---

Medical AI is the use of machine learning and other AI methods in clinical and healthcare contexts, such as imaging, risk prediction, triage, and operational analytics. Compared to many consumer ML settings, medical AI is high stakes, highly regulated, and sensitive to distribution shift and measurement differences across sites.

## Why it matters
- Medical data is often siloed across institutions, devices, and jurisdictions.
- Errors can have real clinical consequences, so evaluation, monitoring, and governance matter.
- Site-to-site differences (population, equipment, coding practices) can be large and must be accounted for.

## In federated settings
Federated AI is often a natural fit for medical collaborations because institutions may be unable to pool raw data centrally. In cross-silo federated learning, each site trains or evaluates locally and contributes constrained outputs (for example [model updates](/glossary/model-update) or locally computed metrics) for aggregation.

Because “privacy” is not automatic, medical federations typically need an explicit [threat model](/glossary/threat-model) and may combine techniques such as [secure aggregation](/glossary/secure-aggregation) and [differential privacy](/glossary/differential-privacy), depending on requirements and who is trusted.

## Common pitfalls
- Treating federated learning as a drop-in privacy solution without specifying the threat model and defenses.
- Inconsistent preprocessing, labeling, or measurement across sites that makes training and evaluation incomparable.
- Reporting only average metrics and missing poor performance on smaller sites or patient cohorts.
- Ignoring non-stationarity (changes in clinical practice or population) that can invalidate historical results.

## In Flower
In Flower, medical collaborations are commonly modeled as [cross-silo federated learning](/glossary/cross-silo-federated-learning) in the [deployment runtime](/glossary/deployment-runtime), where a [ServerApp](/glossary/serverapp) defines a [Strategy](/glossary/strategy) and each site runs a [ClientApp](/glossary/clientapp) under local governance.
