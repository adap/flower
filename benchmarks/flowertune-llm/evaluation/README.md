# FlowerTune LLM Evaluation

This repository provides various evaluation metrics to measure the quality of your finetuned LLMs.
Evaluating your LLM is the final step prior to have your submission added to the [LLM Leaderboard](https://flower.ai/benchmarks/llm-leaderboard#how-to-participate).
The evaluation scores generated here will be displayed as the definitive values on the LLM Leaderboard.

## How to run
Please check out the individual directory corresponding to your selected challenge (general NLP, finance, medical, and code) to learn how to execute the evaluations.

> [!NOTE]  
> If you wish to participate in the LLM Leaderboard, you must not modify the evaluation code and should use the exact command provided in the respective directory to run the evaluation.


## Expected results
The default template generated by `flwr new` for each challenge will produce results as follows, which serve as the lower bound on the LLM Leaderboard.

### General NLP

|          | MT-1 | MT-2 | MT-Avg |  
|:--------:|:----:|:----:|:------:|
| MT Score | 5.54 | 5.52 |  5.53  |

### Finance

|         |  FPB  | FIQA  | TFNS  |  Avg  |  
|:-------:|:-----:|:-----:|:-----:|:-----:|
| Acc (%) | 44.55 | 63.64 | 28.77 | 45.65 |

### Medical

|         | PubMedQA | MedMCQA | MedQA |  Avg  |  
|:-------:|:--------:|:-------:|:-----:|:-----:|
| Acc (%) |  59.00   |  23.69  | 27.10 | 36.60 |

### Code

|            | MBPP  | HumanEval | Multiple (JS) | Multiple (C++) |  Avg  |  
|:----------:|:-----:|:---------:|:-------------:|:--------------:|:-----:|
| Pass@1 (%) | 31.40 |   25.00   |     31.68     |     24.84      | 28.23 |
