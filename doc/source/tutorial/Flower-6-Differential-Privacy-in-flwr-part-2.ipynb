{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Privacy in flwr Part 2\n",
    "## Step by step guide\n",
    "\n",
    "In this tutorial, we will learn the recommended best practices to build an effective differential privacy setting in federated learning using Flower. \n",
    "\n",
    "The documentation will provide a step-by-step guide, code examples, and best practices for implementing user-level differential privacy gurantees in federated learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may consider to look at the previous tutorial where we introduced Differential Privacy in federated learning with Flower ([part 1](Flower-5-Differential-Privacy-in-flwr-part-1.ipynb)), the introductory notebook (again, using [Flower](https://flower.dev/) and [PyTorch](https://pytorch.org/))..\n",
    "\n",
    "\n",
    "> [Star Flower on GitHub](https://github.com/adap/flower) â­ï¸ and join the Flower community on Slack to connect, ask questions, and get help: [Join Slack](https://flower.dev/join-slack) ðŸŒ¼ We'd love to hear from you in the `#introductions` channel! And if anything is unclear, head over to the `#questions` channel.\n",
    "\n",
    "\n",
    "In this notebook, we will demonstrate the recommended best practice for training models with user-level Differential Privacy using.\n",
    " \n",
    "We will train (30 rounds) a model with high trade-off between utility and privacy on the CIFAR-10 training and test set, partition them into ten smaller datasets. If we used more training rounds, we could certainly have a higher-accuracy private model, but not as high as a model trained without DP.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies\n",
    "\n",
    "Next, we install the necessary packages for PyTorch (`torch` and `torchvision`) and Flower (`flwr`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q flwr[simulation] torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all dependencies installed, we can import everything we need for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu using PyTorch 2.0.1+cu117 and Flower 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import flwr as fl\n",
    "import tensorflow_privacy as tfp\n",
    "from flwr.common import Metrics\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Downloading and preprocessing the data\n",
    "\n",
    "We will use a convolutional neural network (CNN) on the popular CIFAR-10 dataset (10 Classes) in a Federated Learning setting of 10 clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.7%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/cifar-10-python.tar.gz to ./dataset\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Setting our constants\n",
    "\n",
    "NUM_CLIENTS = 10\n",
    "BATCH_SIZE = 32\n",
    "NUM_ROUNDS = 15\n",
    "\n",
    "\n",
    "def load_datasets():\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
    "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split training set into 10 partitions to simulate the individual dataset\n",
    "    partition_size = len(trainset) // NUM_CLIENTS\n",
    "    lengths = [partition_size] * NUM_CLIENTS\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Training and Evaluation functions\n",
    "\n",
    "Let's define our model (including `set_parameters` and `get_parameters`), training and test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Strategy customization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP-FedAvg Modification\n",
    "Originally proposed by McMahan, H. Brendan, et al.,\"Learning differentially private recurrent language models\". DP-FedAvg, has been extended by [Andrew et al. 2021, Differentially Private Learning with Adaptive Clipping](https://arxiv.org/abs/1905.03871), is essentially FedAvg with the following modifications:\n",
    "\n",
    "To guarantee DP at a user-level, we need to apply the following modification in our Federated Averaging algorithm:\n",
    "\n",
    " - **Clipping:**, the clients' model updates must be clipped before transmission to the server, bounding the maximum influence of any one client. Moreover, knowing that the distribution of the update norm has been shown to vary from task-to-task and to evolve as training progresses. Therefore, we will use an adaptive approach of [Andrew et al. 2021, Differentially Private Learning with Adaptive Clipping](#dp-fedavg-modification) that continuously adjusts the clipping threshold to track a prespecified quantile of the update norm distribution.\n",
    " - **Noising:** Gaussian noise must be added by either the server or the clients. Adding noise could degrade the utility of the model, but we can control it using   the standard deviation of the Gaussian noise added to the sum, and the number of sampled clients at each round.\n",
    " >>>> *We provide users with the flexibility to set up the training such that each client independently adds a small amount of noise to the clipped update, with the result that simply aggregating the noisy updates is equivalent to the explicit addition of noise to the non-noisy aggregate at the server.*\n",
    "\n",
    "\n",
    " > *Remarks:* In order to do this, we need first to determine how much noise the model can tolerate with the chosed number of clients (relatively small) per round with fair loss to the model utility. We will eventually train the final model with an increased amount of noise with proportional increased number of clients. \n",
    "\n",
    "### Simplifying assumptions for the training process\n",
    "\n",
    "To ensure that the training process realises the $(\\epsilon, \\delta )$-guarantees at the user-level, we made the following assumptions:\n",
    "\n",
    "-**Fixed-size subsampling** :Fixed-size subsamples of the clients must be taken at each round, as opposed to variable-sized Poisson subsamples.\n",
    "\n",
    "-**Unweighted averaging** : The contributions from all the clients must weighted equally in the aggregate to eliminate the requirement for the server to know in advance the sum of the weights of all clients available for selection.\n",
    "\n",
    "-**No client failures** : The set of available clients must stay constant across all rounds of training. In other words, clients cannot drop out or fail.\n",
    "\n",
    "\n",
    " >*Note:* The dataset should be large enough to support the selected number of clients. It is also important to note that since we are using an adaptative clipping, and the noise multiplier being the ratio of the noise standard deviation to the clipping norm; therefore the magnitude of the noise will change from rounds to rounds. \n",
    " >>*The above assumptions are in line with the contraints imposed by [Andrew et al.](#dp-fedavg-modification)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In this training setting, we will use Poison subsampling providing us with a tighter privacy guarantees \n",
    "# Compared to having a a fixed number of clients at each round (which is not always practical). \n",
    "# In a real world setting the number of clients per round is stochatic\n",
    "\n",
    "\n",
    "def train(net, rounds, clients_per_round, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO be customized\n",
    "\n",
    "total_clients = len(train_data.client_ids)\n",
    "\n",
    "def train(rounds, noise_multiplier, clients_per_round, data_frame):\n",
    "  # Using the `dp_aggregator` here turns on differential privacy with adaptive\n",
    "  # clipping.\n",
    "  aggregation_factory = tff.learning.model_update_aggregator.dp_aggregator(\n",
    "      noise_multiplier, clients_per_round)\n",
    "\n",
    "  # We use Poisson subsampling which gives slightly tighter privacy guarantees\n",
    "  # compared to having a fixed number of clients per round. The actual number of\n",
    "  # clients per round is stochastic with mean clients_per_round.\n",
    "  sampling_prob = clients_per_round / total_clients\n",
    "\n",
    "  # Build a federated averaging process.\n",
    "  # Typically a non-adaptive server optimizer is used because the noise in the\n",
    "  # updates can cause the second moment accumulators to become very large\n",
    "  # prematurely.\n",
    "  learning_process = tff.learning.algorithms.build_unweighted_fed_avg(\n",
    "        my_model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(0.01),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0, momentum=0.9),\n",
    "        model_aggregator=aggregation_factory)\n",
    "\n",
    "  eval_process = tff.learning.build_federated_evaluation(my_model_fn)\n",
    "    \n",
    "  # Training loop.\n",
    "  state = learning_process.initialize()\n",
    "  for round in range(rounds):\n",
    "    if round % 5 == 0:\n",
    "      model_weights = learning_process.get_model_weights(state)\n",
    "      metrics = eval_process(model_weights, [test_data])['eval']\n",
    "      if round < 25 or round % 25 == 0:\n",
    "        print(f'Round {round:3d}: {metrics}')\n",
    "      data_frame = data_frame.append({'Round': round,\n",
    "                                      'NoiseMultiplier': noise_multiplier,\n",
    "                                      **metrics}, ignore_index=True)\n",
    "\n",
    "    # Sample clients for a round. Note that if your dataset is large and\n",
    "    # sampling_prob is small, it would be faster to use gap sampling.\n",
    "    x = np.random.uniform(size=total_clients)\n",
    "    sampled_clients = [\n",
    "        train_data.client_ids[i] for i in range(total_clients)\n",
    "        if x[i] < sampling_prob]\n",
    "    sampled_train_data = [\n",
    "        train_data.create_tf_dataset_for_client(client)\n",
    "        for client in sampled_clients]\n",
    "\n",
    "    # Use selected clients for update.\n",
    "    result = learning_process.next(state, sampled_train_data)\n",
    "    state = result.state\n",
    "    metrics = result.metrics\n",
    "\n",
    "  model_weights = learning_process.get_model_weights(state)\n",
    "  metrics = eval_process(model_weights, [test_data])['eval']\n",
    "  print(f'Round {rounds:3d}: {metrics}')\n",
    "  data_frame = data_frame.append({'Round': rounds,\n",
    "                                  'NoiseMultiplier': noise_multiplier,\n",
    "                                  **metrics}, ignore_index=True)\n",
    "\n",
    "  return data_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the *$(\\epsilon, \\delta )$-analysis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_order = 32\n",
    "orders = range(2, max_order + 1)\n",
    "rdp = tfp.compute_rdp_sample_without_replacement(q, z, n, orders)\n",
    "eps, _, _ = tfp.rdp_accountant.get_privacy_spent(rdp, target_delta=delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
