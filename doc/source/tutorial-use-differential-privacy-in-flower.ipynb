{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Privacy in flwr Part 2\n",
    "## Step by step guide\n",
    "\n",
    "In this tutorial, we will learn the recommended best practices to build an effective differential privacy setting in federated learning using Flower. \n",
    "\n",
    "The documentation will provide a step-by-step guide, code examples, and best practices for implementing user-level differential privacy gurantees in federated learning systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may consider to look at the previous tutorial where we introduced Differential Privacy in federated learning with Flower ([part 1](Flower-5-Differential-Privacy-in-flwr-part-1.ipynb)), the introductory notebook (again, using [Flower](https://flower.dev/) and [PyTorch](https://pytorch.org/))..\n",
    "\n",
    "\n",
    "> [Star Flower on GitHub](https://github.com/adap/flower) â­ï¸ and join the Flower community on Slack to connect, ask questions, and get help: [Join Slack](https://flower.dev/join-slack) ðŸŒ¼ We'd love to hear from you in the `#introductions` channel! And if anything is unclear, head over to the `#questions` channel.\n",
    "\n",
    "\n",
    "In this notebook, we will demonstrate the recommended best practice for training models with user-level Differential Privacy using.\n",
    " \n",
    "We will train (30 rounds) a model with high trade-off between utility and privacy on the CIFAR-10 training and test set, partition them into ten smaller datasets. If we used more training rounds, we could certainly have a higher-accuracy private model, but not as high as a model trained without DP.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies\n",
    "\n",
    "Next, we install the necessary packages for PyTorch (`torch` and `torchvision`) and Flower (`flwr`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -e ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To launch first \n",
    "%pip install flwr[simulation]==1.5.0 torch torchvision matplotlib -e ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install  flwr[simulation] torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all dependencies installed, we can import everything we need for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib torch torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import flwr as fl\n",
    "    #import tensorflow_privacy as tfp\n",
    "from flwr.common import Metrics\n",
    "from flwr.server.history import History\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Downloading and preprocessing the data\n",
    "\n",
    "We will use a convolutional neural network (CNN) on the popular CIFAR-10 dataset (10 Classes) in a Federated Learning setting of 50 clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting our constants\n",
    "\n",
    "NUM_CLIENTS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "def load_datasets(num_clients: int):\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
    "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split training set into 10 partitions to simulate the individual dataset\n",
    "    partition_size = len(trainset) // num_clients\n",
    "    lengths = [partition_size] * num_clients\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=BATCH_SIZE))\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Training and Evaluation functions\n",
    "\n",
    "Let's define our model (including `set_parameters` and `get_parameters`), training and test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flower client\n",
    "\n",
    "We create a subclass of `flwr.client.DPFedAvgNumPyClient` and implement the three methods `get_parameters`, `fit`, and `evaluate`. Here, we also pass the `cid` to the client and use it log additional details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient (fl.client.DPFedAvgNumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "    \n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    net = Net().to(DEVICE)\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Strategy customization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplifying Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DP-FedAvg Modification\n",
    "Originally proposed by McMahan, H. Brendan, et al.,\"Learning differentially private recurrent language models\". DP-FedAvg, has been extended by [Andrew et al. 2021, Differentially Private Learning with Adaptive Clipping](https://arxiv.org/abs/1905.03871), is essentially FedAvg with the following modifications:\n",
    "\n",
    "To guarantee DP at a user-level, we need to apply the following modification in our Federated Averaging algorithm:\n",
    "\n",
    " - **Clipping:**, the clients' model updates must be clipped before transmission to the server, bounding the maximum influence of any one client. Moreover, knowing that the distribution of the update norm has been shown to vary from task-to-task and to evolve as training progresses. Therefore, we will use an adaptive approach of [Andrew et al. 2021, Differentially Private Learning with Adaptive Clipping](#dp-fedavg-modification) that continuously adjusts the clipping threshold to track a prespecified quantile of the update norm distribution.\n",
    " - **Noising:** Gaussian noise must be added by either the server or the clients. Adding noise could degrade the utility of the model, but we can control it using   the standard deviation of the Gaussian noise added to the sum, and the number of sampled clients at each round.\n",
    " >>>> *We provide users with the flexibility to set up the training such that each client independently adds a small amount of noise to the clipped update, with the result that simply aggregating the noisy updates is equivalent to the explicit addition of noise to the non-noisy aggregate at the server.*\n",
    "\n",
    "\n",
    " > *Remarks:* In order to do this, we need first to determine how much noise the model can tolerate with the chosed number of clients (relatively small) per round with fair loss to the model utility. We will eventually train the final model with an increased amount of noise with proportional increased number of clients. \n",
    "\n",
    "### Simplifying assumptions for the training process\n",
    "\n",
    "To ensure that the training process realises the $(\\epsilon, \\delta )$-guarantees at the user-level, we made the following assumptions:\n",
    "\n",
    "-**Fixed-size subsampling** :Fixed-size subsamples of the clients must be taken at each round, as opposed to variable-sized Poisson subsamples.\n",
    "\n",
    "-**Unweighted averaging** : The contributions from all the clients must weighted equally in the aggregate to eliminate the requirement for the server to know in advance the sum of the weights of all clients available for selection.\n",
    "\n",
    "-**No client failures** : The set of available clients must stay constant across all rounds of training. In other words, clients cannot drop out or fail.\n",
    "\n",
    "\n",
    " >*Note:* The dataset should be large enough to support the selected number of clients. It is also important to note that since we are using an adaptative clipping, and the noise multiplier being the ratio of the noise standard deviation to the clipping norm; therefore the magnitude of the noise will change from rounds to rounds. \n",
    " >>*The above assumptions are in line with the contraints imposed by [Andrew et al.](#dp-fedavg-modification)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Determine the noise sensitivity of the model\n",
    "\n",
    "For our setting we have 50 clients (each holding 45 training and 5 validation examples). The number of training examples on each client being very small, let's configure the clients to perform 3 local training epoch. We also adjust the fraction of clients selected for training during each round (we don't want all 50 clients participating in every round), so we adjust `fraction_fit` to `0.5`, which means that only 50% of available clients (25 clients) will be selected for training each round.\n",
    "\n",
    "\n",
    "> *NB:* All these specifications will be wrapped in a customized function called `dp_experiment` for a proper and easy computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric aggregation function\n",
    "def unweighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    # Multiply accuracy of each client by a neutral number of example\n",
    "    num_examples = 1\n",
    "    accuracies = [num_examples * m[\"accuracy\"] for _ , m in metrics]\n",
    "    examples = [num_examples for _ , _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
    "\n",
    "\n",
    "#Configure the clients to perform 3 local training epoch\n",
    "def fit_config(server_round: int):\n",
    "    config = {\n",
    "        \"server_round\": server_round,\n",
    "        \"local_epochs\": 3,\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def dp_experiment(NUM_CLIENTS, NUM_ROUNDS, noise_multiplier)-> History:\n",
    "    \n",
    "    strategy1 = fl.server.strategy.FedAvg(\n",
    "        fraction_fit=0.5, # Train on 50 clients (each round)\n",
    "        fraction_evaluate=0.25,  # Evaluate on 25 clients (each round)\n",
    "        min_fit_clients=3,\n",
    "        min_evaluate_clients=3,\n",
    "        min_available_clients=NUM_CLIENTS,\n",
    "        evaluate_metrics_aggregation_fn= unweighted_average,\n",
    "        on_fit_config_fn=fit_config,\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Net())),\n",
    "    )\n",
    "\n",
    "    strategy = fl.server.strategy.DPFedAvgAdaptive(\n",
    "        strategy = strategy1 ,\n",
    "        num_sampled_clients =  strategy1.num_fit_clients(NUM_CLIENTS)[0], \n",
    "        init_clip_norm = 0.3 ,\n",
    "        noise_multiplier = noise_multiplier, # Must be carefully chose accordingly to the clip_count_stddev bellow\n",
    "        server_side_noising = False ,\n",
    "        clip_norm_target_quantile = 0.5 ,\n",
    "        clip_count_stddev = None,  # if not specified, it takes the default value 'self.num_sampled_clients` / 20.0\n",
    "    )\n",
    "\n",
    "    # Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "    client_resources = {\"num_cpus\": 1, \"num_gpus\": 0}\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        client_resources = {\"num_gpus\": 1}\n",
    "\n",
    "    # Start simulation\n",
    "    return fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients= NUM_CLIENTS,\n",
    "        config=fl.server.ServerConfig(NUM_ROUNDS),  # Just three rounds\n",
    "        strategy=strategy,\n",
    "        client_resources=client_resources,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = []\n",
    "NUM_ROUNDS = 5\n",
    "\n",
    "for noise_multiplier in [0.0, 0.5, 0.75, 1.0]:\n",
    "  print(f'Starting training with noise multiplier: {noise_multiplier}')\n",
    "  hist = dp_experiment(NUM_CLIENTS, NUM_ROUNDS, noise_multiplier)\n",
    "  record.append({\"Noise multiplier\": noise_multiplier, \"History\": hist})\n",
    "# print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i['History'].metrics_distributed for i in record])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the *$(\\epsilon, \\delta )$-analysis for a scaled FL*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two metrics are used to express the DP guarantee of an ML algorithm:\n",
    "\n",
    "1.   Epsilon ($\\epsilon$) - This is the privacy budget. It measures the strength of the privacy guarantee by bounding how much the probability of a particular model output can vary by including (or excluding) a single training point. A smaller value for $\\epsilon$ implies a better privacy guarantee. However, the $\\epsilon$ value is only an upper bound and a large value could still mean good privacy in practice.\n",
    "\n",
    "2. Delta ($\\delta$) - Bounds the probability of the privacy guarantee not holding. A general rule is to set it to be less than the inverse of the size of the training dataset. we set it to **10^-4** as the CIFAR10 dataset has 50,000 training points.\n",
    "\n",
    "\n",
    "> We use `dp_accounting.calibrate_dp_mechanism` to search over the number of clients per round. The privacy accountant (`RdpAccountant`) we use to estimate privacy given a `dp_accounting.DpEvent` is based on [Wang et al. (2018)](https://arxiv.org/abs/1808.00087) and [Mironov et al. (2019)](https://arxiv.org/pdf/1908.10530.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade dp-accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dp_accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clients = 50000\n",
    "noise_to_clients_ratio = 0.005\n",
    "target_delta = 1e-4\n",
    "target_eps = 2\n",
    "rounds = 100\n",
    "\n",
    "# Initialize arguments to dp_accounting.calibrate_dp_mechanism.\n",
    "\n",
    "# No-arg callable that returns a fresh accountant.\n",
    "make_fresh_accountant = dp_accounting.rdp.RdpAccountant\n",
    "\n",
    "# Create function that takes expected clients per round and returns a \n",
    "# dp_accounting.DpEvent representing the full training process.\n",
    "def make_event_from_param(clients_per_round):\n",
    "  q = clients_per_round / total_clients\n",
    "  noise_multiplier = clients_per_round * noise_to_clients_ratio\n",
    "  gaussian_event = dp_accounting.GaussianDpEvent(noise_multiplier)\n",
    "  sampled_event = dp_accounting.PoissonSampledDpEvent(q, gaussian_event)\n",
    "  composed_event = dp_accounting.SelfComposedDpEvent(sampled_event, rounds)\n",
    "  return composed_event\n",
    "\n",
    "# Create object representing the search range [1, 3383].\n",
    "bracket_interval = dp_accounting.ExplicitBracketInterval(1, total_clients)\n",
    "\n",
    "# Perform search for smallest clients_per_round achieving the target privacy.\n",
    "clients_per_round = dp_accounting.calibrate_dp_mechanism(\n",
    "    make_fresh_accountant, make_event_from_param, target_eps, target_delta,\n",
    "    bracket_interval, discrete=True\n",
    ")\n",
    "\n",
    "noise_multiplier = clients_per_round * noise_to_clients_ratio\n",
    "print(f'To get ({target_eps}, {target_delta})-DP, use {clients_per_round} '\n",
    "      f'clients with noise multiplier {noise_multiplier}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
